{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D functions used to compute targets\n",
    "def sin_sqrt(X):\n",
    "    return np.sin(np.sqrt(X[:, 0]**2 + X[:, 1]**2))\n",
    "\n",
    "def booth(X):\n",
    "    X1, X2 = X[:, 0], X[:, 1]\n",
    "    return ((X1 + 2*X2 - 7)**2 + (2*X1 + X2 - 5)**2)\n",
    "\n",
    "def goldstein_price(X):\n",
    "    X1, X2 = X[:, 0], X[:, 1]\n",
    "    return ((1+(X1 + X2 + 1)**2*(19-14*X1+3*X1**2-14*X2+6*X1*X2+3*X2**2))*(30+(2*X1-3*X2)**2*(18-32*X1+12*X1**2+48*X2-36*X1*X2+27*X2**2)))\n",
    "\n",
    "def beale(X):\n",
    "    X1, X2 = X[:, 0], X[:, 1]\n",
    "    return (1.5 - X1 + X1*X2)**2 + (2.25 - X1 + X1*X2**2)**2 + (2.625 - X1 + X1*X2**3)**2\n",
    "\n",
    "# function to generate 2D input data\n",
    "def generate_X(n_samples, ranges=[(0, 1), (0, 1)], random_state=None):\n",
    "    dim = len(ranges)\n",
    "    assert dim > 0\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    X = np.zeros((n_samples, dim))\n",
    "    for i, r in enumerate(ranges):\n",
    "        low, high = r\n",
    "        X[:, i] = rng.uniform(low, high, n_samples)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate datasets\n",
    "seed = 42\n",
    "n_samples = 10000\n",
    "features = dict(\n",
    "    sin_sqrt=generate_X(n_samples, ranges=[(-5, 5), (-5, 5)], random_state=seed),\n",
    "    booth=generate_X(n_samples, ranges=[(-10, 10), (-10, 10)], random_state=seed),\n",
    "    goldstein_price=generate_X(n_samples, ranges=[(-2, 2), (-2, 2)], random_state=seed),\n",
    "    beale=generate_X(n_samples, ranges=[(-4.5, 4.5), (-4.5, 4.5)], random_state=seed)\n",
    ")\n",
    "\n",
    "targets = dict(\n",
    "    sin_sqrt=sin_sqrt(features['sin_sqrt']),\n",
    "    booth=booth(features['booth']),\n",
    "    goldstein_price=goldstein_price(features['goldstein_price']),\n",
    "    beale=beale(features['beale'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "fn_map = dict(\n",
    "    goldstein_price=\"Goldstein Price\",\n",
    "    booth=\"Booth\",\n",
    "    beale=\"Beale\",\n",
    "    sin_sqrt=\"SSR\"\n",
    ")\n",
    "\n",
    "def plot_as_heatmap(ax, fig, x, y, z, n_points=100, cmap=\"viridis\", norm=\"symlog\", interpolation=\"cubic\"):\n",
    "    x_min, x_max = min(x), max(x)\n",
    "    y_min, y_max = min(y), max(y)\n",
    "    X, Y = np.meshgrid(np.linspace(x_min, x_max, n_points), np.linspace(y_min, y_max, n_points))\n",
    "    Z = griddata((x, y), z, (X, Y), method=interpolation)\n",
    "    pcm = ax.imshow(Z, extent=[x_min, x_max, y_min, y_max], origin=\"lower\", cmap=cmap, norm=norm)\n",
    "    return pcm\n",
    "\n",
    "n_fns = len(targets)\n",
    "n_rows = 2\n",
    "height = 5\n",
    "fig = plt.figure(figsize=(height*n_rows, height*n_rows))\n",
    "for idx, fn in enumerate(features):\n",
    "    ax = fig.add_subplot(n_rows, n_fns//n_rows, idx+1)\n",
    "    pcm = plot_as_heatmap(ax, fig, features[fn][:, 0], features[fn][:, 1], targets[fn], n_points=1000)\n",
    "    ax.set_title(fn_map[fn])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train = {}\n",
    "test = {}\n",
    "scalers = {}\n",
    "for k in features:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features[k], targets[k], test_size=0.2)\n",
    "    train[k] = (X_train, y_train)\n",
    "    test[k] = (X_test, y_test)\n",
    "    scalers[k] = MinMaxScaler((-1, 1))\n",
    "    scalers[k].fit(X_train)\n",
    "scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 K-Fold Cross-Validation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_squared_error, mean_absolute_error\n",
    "from mas import context\n",
    "from mas import head\n",
    "\n",
    "\n",
    "# loading best params for benchmark algorithms\n",
    "with open(\"best_params.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "best_params[\"mas\"] = dict(\n",
    "    sin_sqrt=dict(\n",
    "        R=np.full(2, 0.1),\n",
    "        alpha=0.4,\n",
    "        bad_th=0.2,\n",
    "        imprecise_th=0.01,\n",
    "        memory_length=20,\n",
    "        min_vol=0.\n",
    "    ),\n",
    "    booth=dict(\n",
    "        R=np.full(2, 0.35),\n",
    "        alpha=0.1,\n",
    "        bad_th=0.2,\n",
    "        imprecise_th=0.15,\n",
    "        memory_length=50,\n",
    "        min_vol=0.\n",
    "    ),\n",
    "    goldstein_price=dict(\n",
    "        R=np.full(2, 0.1),\n",
    "        alpha=0.1,\n",
    "        bad_th=0.2,\n",
    "        imprecise_th=0.01,\n",
    "        memory_length=200,\n",
    "        min_vol=0.\n",
    "    ),\n",
    "    beale=dict(\n",
    "        R=np.full(2, 0.05),\n",
    "        alpha=0.2,\n",
    "        bad_th=0.3,\n",
    "        imprecise_th=0.1,\n",
    "        memory_length=20,\n",
    "        min_vol=0.\n",
    "    ),\n",
    ")\n",
    "\n",
    "# initialize models from best params:\n",
    "models_cls = dict(\n",
    "    xgboost=XGBRegressor,\n",
    "    lightbgm=LGBMRegressor,\n",
    "    random_forest=RandomForestRegressor,\n",
    "    decision_tree=DecisionTreeRegressor,\n",
    "    linear_regression=LinearRegression,\n",
    "    gradient_boosting=GradientBoostingRegressor,\n",
    "    svr=SVR,\n",
    "    mas=head.FastHeadAgent\n",
    ")\n",
    "models = {}\n",
    "for k in best_params:\n",
    "    models_by_fn = {}\n",
    "    algo = models_cls[k]\n",
    "    for fn, params in best_params[k].items():\n",
    "        models_by_fn[fn] = algo(**params)\n",
    "    models[k] = models_by_fn\n",
    "models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate cross validation score\n",
    "cross_val_results = {}\n",
    "scoring = {\n",
    "    'r2': make_scorer(r2_score),\n",
    "    'mse': make_scorer(mean_squared_error),\n",
    "    'mae': make_scorer(mean_absolute_error)\n",
    "}\n",
    "pbar = tqdm.tqdm(models)\n",
    "for m in pbar:\n",
    "    res = {}\n",
    "    for fn, model in models[m].items():\n",
    "        pbar.set_description(f\"[{m} - {fn}]\")\n",
    "        X_train, y_train = train[fn]\n",
    "        score = cross_validate(model, scalers[fn].transform(X_train), y_train, cv=5, scoring=scoring, return_estimator=True, n_jobs=-1)\n",
    "        res[fn] = score\n",
    "    cross_val_results[m] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "scores = {k: v for k,v in cross_val_results.items()}\n",
    "scores = {k: {fn: {metric: values.mean() for metric, values in m.items() if metric.startswith(\"test_\")} for fn, m in v.items()} for k,v in scores.items()}\n",
    "results_df = pd.DataFrame.from_dict({(outerKey, innerKey): values for outerKey, innerDict in scores.items() for innerKey, values in innerDict.items()}, orient='index')\n",
    "results_df = results_df.swaplevel().sort_index()\n",
    "results_df.to_excel(\"results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Comparing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# evaluate on test data\n",
    "results = dict()\n",
    "\n",
    "for fn in test:\n",
    "    X_test, y_test = test[fn]\n",
    "    res = {}\n",
    "    for m in models:\n",
    "        model = models[m][fn]\n",
    "        y_pred = model.predict(scalers[fn].transform(X_test))\n",
    "        mse = mean_squared_error(y_pred, y_test)\n",
    "        mae = mean_absolute_error(y_pred, y_test)\n",
    "        r2 = r2_score(y_pred, y_test)\n",
    "        res[m] = dict(\n",
    "            mse=mse, \n",
    "            mae=mae,\n",
    "            r2=r2,\n",
    "        )\n",
    "    results[fn] = res\n",
    "\n",
    "results_df = pd.DataFrame.from_dict({(outerKey, innerKey): values for outerKey, innerDict in results.items() for innerKey, values in innerDict.items()}, orient='index')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ms = [\"mas\", \"xgboost\", \"lightbgm\"]\n",
    "n_models = len(ms)\n",
    "n_fns = len(test)\n",
    "height = 6\n",
    "width = 5\n",
    "n_cols = n_models + 1\n",
    "n_rows = n_fns\n",
    "\n",
    "fig = plt.figure(figsize=(height * n_cols, width * n_rows), layout=\"constrained\")\n",
    "plt_idx = 0\n",
    "for fn in test:\n",
    "    X_test, y_test = test[fn]\n",
    "    v_min, v_max = np.round(y_test.min()), np.round(y_test.max())\n",
    "    \n",
    "    for m in ms:\n",
    "        plt_idx += 1\n",
    "        y_pred = models[m][fn].predict(scalers[fn].transform(X_test))\n",
    "        ax = fig.add_subplot(n_cols, n_rows, plt_idx)\n",
    "        pcm_ = plot_as_heatmap(ax, fig, X_test[:, 0], X_test[:, 1], y_pred, n_points=1000, interpolation=\"linear\")\n",
    "        pcm_.set_clim(vmin=v_min, vmax=v_max)\n",
    "        ax.set_title(f\"prediction {m}\")\n",
    "    fig.colorbar(pcm_)\n",
    "    \n",
    "    plt_idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
